# Flash Sale Event Stream Processor

This project is a streaming data processor that reads events generated by a real-time user interation with an ecommerce platform. The data generated is processed and aggregated in real time with the usage of the OpenSource Big Data Process Apache Flink.
For this Flink Job, the source was mocked with a custom data generator and the sink was improvised in the application console.

Because of the development in Windows machine, I let it in the project the docker-compose used to deploy the flink jobs. There is mounted in this docker the target folder containing the JAR file to allow for job submission using the Flink CLI, and also an extra data volume in case of the need of tests with different sources and also for the development of batch processing applications.

The sink can be seen in the terminal of the task manager!

## Project Structure

```bash
.
├── src/
│   └── org/
│       └── streaming/
│           └── job/
│               ├── FlashSaleEvent.java
│               ├── FlashSaleEventSource.java
│               ├── AggregatedFlashSaleEvent.java
│               └── Main.java
├── .env
├── docker-compose.yml
├── pom.xml
├── README.md
└── target/
```

## Description of Files:
- FlashSaleEvent.java: POJO with the events to be processed.

- FlashSaleEventSource.java: Flink stream source mocking data being read from a Kafka topic or an Eventhub.

- AggregatedFlashSaleEvent.java: POJO with the event structure after the aggregation.

- Main.java: Entry point of the Flink Application. It reads from the streaming source, apply a groupby by clientId, sum TotalAmount and sum Units. The sink is the print in the console.